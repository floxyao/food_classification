{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2grey\n",
    "\n",
    "# Library for scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Library for LIBSVM\n",
    "from libsvm.svmutil import *\n",
    "from itertools import combinations\n",
    "from skimage.data import camera\n",
    "\n",
    "# Library fror tensorflow and keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 101 is main dataset, 102 is for testing with smaller dataset\n",
    "data_dir = \"/mnt/c/Users/nhmin/Downloads/food-101/\"\n",
    "bin_n = 16 # Number of bin\n",
    "project_dir = os.getcwd()\n",
    "model_dir = \"/mnt/c/Users/nhmin/Downloads/\"\n",
    "class_label = {\"pad_thai\" : 0, \"pho\" : 1, \"ramen\" : 2, \"spaghetti_bolognese\" : 3, \"spaghetti_carbonara\" : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet_model():\n",
    "    # Start off the model\n",
    "    model = Sequential()\n",
    "    model.trainable = False\n",
    "    # 1st Convolution layer\n",
    "    model.add(Conv2D(input_shape=(227,227,3), filters=96,kernel_size=(11,11),strides=(4,4),padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # max pooling 1sst layer\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # 2nd Convolution layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # max pooling for 2nd layer\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    model.add(BatchNormalization())\n",
    "    #===================\n",
    "    # Testing to get layer 2\n",
    "    layer = model.get_layer(index=2)\n",
    "    print(layer.get_output_at(0))\n",
    "    #===================\n",
    "    # 3rd Convolution layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # 4th Convolution layer\n",
    "    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    # 5th Convolution layer\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
    "    model.add(Activation('relu'))\n",
    "    # max pooling for 5th layer\n",
    "    model.add(MaxPooling2D(pool_size=(3,3), strides=(2,2), padding='valid'))\n",
    "    \n",
    "    # dense layer\n",
    "    model.add(Flatten())\n",
    "    # 1st dense layer\n",
    "    model.add(Dense(4096, input_shape=(6*6*256,)))\n",
    "    model.add(Activation('relu'))\n",
    "    # dropout to prevent overfitting\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # 2nd dense layer\n",
    "    model.add(Dense(4096))\n",
    "    model.add(Activation('relu'))\n",
    "    # drop out to prevent overfitting\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    # 3rd dense layer\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    # output layer\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Uncomment line below to see architecture detail\n",
    "    # model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return np img size 227x227\n",
    "def get_image(path):\n",
    "    # image resize to 227x227\n",
    "    img = Image.open(path)\n",
    "    resized_image = img.resize((227,227), Image.ANTIALIAS)\n",
    "    return np.array(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    final_data = dict()\n",
    "    # Load in json file to create dictionary: key = class label; value = file path\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Only get information from needed class\n",
    "    for label in class_label:\n",
    "        final_data.update({label : data.get(label)})\n",
    "    return final_data\n",
    "\n",
    "#=========================\n",
    "# Exclusive block for showing a sample of how json load data\n",
    "# data_head = 5\n",
    "# sample_data = load_json(parent_dir + \"/meta/test.json\")\n",
    "# for key, value in sample_data:\n",
    "#     print(key)\n",
    "#     print(value[i] for i in range(0,data_head))\n",
    "#     for i in range(0, data_head):\n",
    "#         print(value[i])   \n",
    "#========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def HOG_image(image):\n",
    "#     np_image = get_image(parent_dir + \"images/\" + image)\n",
    "#     # given 32x32 cell\n",
    "#     image_feature, image_hog = hog(np_image, orientations=8, pixels_per_cell=(8, 8),\n",
    "#         cells_per_block=(8, 8), block_norm = 'L2-Hys', visualize=True, multichannel=True)\n",
    "#     return np.array(image_feature)\n",
    "\n",
    "# def pca_transform(image):\n",
    "#     ss = StandardScaler()\n",
    "#     image_ss = ss.fit_transform(image)\n",
    "#     # Keep 90% of variance\n",
    "#     pca = PCA(0.85)\n",
    "#     image_pca = pca.fit_transform(image_ss)\n",
    "#     return image_pca\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in class_label.items():\n",
    "        if(val == value): return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(json_data):\n",
    "    # Create train data\n",
    "    file_names_list = []\n",
    "    label_list = []\n",
    "    for label in json_data.keys():\n",
    "        file_names = os.listdir(data_dir + '/images/' + label)\n",
    "        insert_pos = len(file_names)\n",
    "        for file in file_names:\n",
    "            file_names_list.append(file)\n",
    "            label_list.append(class_label.get(label))\n",
    "    # Create dataframe\n",
    "    data_df = pd.DataFrame({\n",
    "        'filename' : file_names_list,\n",
    "        'label' : label_list\n",
    "    })\n",
    "#     # Splitting training and validating data\n",
    "#     train_df, validate_df = train_test_split(data_df, test_size=0.20, random_state=42)\n",
    "#     # When splitting, train_df and validate_df contain original index. this is to reset the index\n",
    "#     train_df = train_df.reset_index(drop=True)\n",
    "#     validate_df = validate_df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(data_df, pre_model):\n",
    "    x_tmp = []\n",
    "    size = len(data_df['filename'])\n",
    "    for i in range(0,size):\n",
    "        image = data_df['filename'][i]\n",
    "        label = data_df['label'][i]\n",
    "        path = data_dir + 'images/' + get_key(label) + '/' + image\n",
    "        #image_np = get_image(path)\n",
    "        image_np = load_img(path, target_size=(227,227))\n",
    "        image_np = img_to_array(image_np)\n",
    "        image_np = np.expand_dims(image_np,axis=0)\n",
    "        image_np = imagenet_utils.preprocess_input(image_np)\n",
    "        x_tmp.append(image_np)\n",
    "    x = np.vstack(x_tmp)\n",
    "    features = pre_model.predict(x, batch_size=32)\n",
    "    features_flatten = features.reshape((features.shape[0], 2*2*256))\n",
    "    return features_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 25, 25, 256)       221440    \n",
      "_________________________________________________________________\n",
      "activation_55 (Activation)   (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 8, 8, 384)         2457984   \n",
      "_________________________________________________________________\n",
      "activation_56 (Activation)   (None, 8, 8, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_33 (Conv2D)           (None, 6, 6, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_57 (Activation)   (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 4, 4, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_58 (Activation)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 2, 2, 256)         0         \n",
      "=================================================================\n",
      "Total params: 4,928,256\n",
      "Trainable params: 0\n",
      "Non-trainable params: 4,928,256\n",
      "_________________________________________________________________\n",
      "0       0\n",
      "1       0\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "4995    4\n",
      "4996    4\n",
      "4997    4\n",
      "4998    4\n",
      "4999    4\n",
      "Name: label, Length: 5000, dtype: int64\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# load json file\n",
    "train_json = load_json(data_dir + '/meta/train.json')\n",
    "train_df = create_data(train_json)\n",
    "\n",
    "#model = tf.keras.models.load_model(model_dir + '227-imgsz-32-bsz-0.01-lr-30-ep/saved_model.pb')\n",
    "model = tf.keras.models.load_model(model_dir + '227-imgsz-32-bsz-0.01-lr-30-ep.ckpt.02-1.64.hdf5')\n",
    "drop_layer = 12\n",
    "new_model = tf.keras.Sequential()\n",
    "for layer in model.layers[:-drop_layer]:\n",
    "    new_model.add(layer)\n",
    "new_model.trainable = False\n",
    "new_model.summary()\n",
    "# change optimizer when get from flo\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "train_feature = create_feature(train_df, new_model)\n",
    "print(train_df['label'])\n",
    "print(type(train_df['label']))\n",
    "# b = 1 will give prob est\n",
    "train_model = svm_train(train_df['label'].tolist(), train_feature, '-s 0 -t 0 -c 2.3 -b 1')\n",
    "svm_save_model('all_food_classification.model', train_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset():\n",
    "    model = svm_load_model('all_food_classification.model')\n",
    "    test_json = load_json(data_dir + 'meta/test.json')\n",
    "    test_df = create_data(test_json)\n",
    "    test_feature = create_feature(test_df, new_model)\n",
    "    \n",
    "    # Predict\n",
    "    p_label, p_acc, p_val = svm_predict(test_label, test_data, model, '-b 0')\n",
    "    acc, mse, scc = evaluations(test_label, p_label)\n",
    "    print(\"Test acc: \", acc)\n",
    "    # print wront classify image\n",
    "    counter = 0\n",
    "    for i in range(0, len(p_label)):\n",
    "        if(p_label[i] != test_df['label'][i]):\n",
    "            print(test_df['filename'][i])\n",
    "            print('True label: ', get_key(test_df['label'][i]), \" Predicted label: \", get_key(p_label[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
