{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2grey\n",
    "\n",
    "# Library for scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Library for LIBSVM\n",
    "from libsvm.svmutil import *\n",
    "from itertools import combinations\n",
    "from skimage.data import camera\n",
    "\n",
    "# Library fror tensorflow and keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 101 is main dataset, 102 is for testing with smaller dataset\n",
    "data_dir = \"/mnt/c/Users/nhmin/Downloads/food-101\"\n",
    "bin_n = 16 # Number of bin\n",
    "project_dir = os.getcwd()\n",
    "model_dir = \"/mnt/c/Users/nhmin/Downloads\"\n",
    "class_label = {\"pad_thai\" : 0, \"pho\" : 1, \"ramen\" : 2, \"spaghetti_bolognese\" : 3, \"spaghetti_carbonara\" : 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    final_data = dict()\n",
    "    # Load in json file to create dictionary: key = class label; value = file path\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Only get information from needed class\n",
    "    for label in class_label:\n",
    "        final_data.update({label : data.get(label)})\n",
    "    return final_data\n",
    "\n",
    "def get_key(val):\n",
    "    for key, value in class_label.items():\n",
    "        if(val == value): return key\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(json_data):\n",
    "    # Create train data\n",
    "    file_names_list = []\n",
    "    label_list = []\n",
    "    for label in json_data.keys():\n",
    "        #file_names = os.listdir(data_dir + '/images/' + label)\n",
    "        file_names = json_data.get(label)\n",
    "        for file in file_names:\n",
    "            file = file.split('/')[1] + '.jpg'\n",
    "            file_names_list.append(file)\n",
    "            label_list.append(class_label.get(label))\n",
    "    # Create dataframe\n",
    "    data_df = pd.DataFrame({\n",
    "        'filename' : file_names_list,\n",
    "        'label' : label_list\n",
    "    })\n",
    "#     # Splitting training and validating data\n",
    "#     train_df, validate_df = train_test_split(data_df, test_size=0.20, random_state=42)\n",
    "#     # When splitting, train_df and validate_df contain original index. this is to reset the index\n",
    "#     train_df = train_df.reset_index(drop=True)\n",
    "#     validate_df = validate_df.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature(data_df, pre_model):\n",
    "    x_tmp = []\n",
    "    size = len(data_df['filename'])\n",
    "    print(\"total image: \", size)\n",
    "    for i in range(0,size):\n",
    "        image = data_df['filename'][i]\n",
    "        label = data_df['label'][i]\n",
    "        path = data_dir + '/images/' + get_key(label) + '/' + image\n",
    "        #image_np = get_image(path)\n",
    "        image_np = load_img(path, target_size=(227,227))\n",
    "        image_np = img_to_array(image_np)\n",
    "        image_np = np.expand_dims(image_np,axis=0)\n",
    "        image_np = imagenet_utils.preprocess_input(image_np)\n",
    "        x_tmp.append(image_np)\n",
    "    x = np.vstack(x_tmp)\n",
    "    features = pre_model.predict(x, batch_size=32)\n",
    "    last_layer = pre_model.layers[-1].output_shape\n",
    "    print(\"Reshape:\", features.shape[0], last_layer[1] * last_layer[2] * last_layer[3])\n",
    "    features_flatten = features.reshape((features.shape[0], last_layer[1] * last_layer[2] * last_layer[3]))\n",
    "    return features_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 55, 55, 96)        384       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 25, 25, 256)       221440    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 25, 25, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 10, 10, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 5, 5, 256)         0         \n",
      "=================================================================\n",
      "Total params: 846,848\n",
      "Trainable params: 0\n",
      "Non-trainable params: 846,848\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load json file\n",
    "# train_json = load_json(data_dir + '/meta/train.json')\n",
    "# train_df = create_data(train_json)\n",
    "\n",
    "#model = tf.keras.models.load_model(model_dir + '227-imgsz-32-bsz-0.01-lr-30-ep/saved_model.pb')\n",
    "model = tf.keras.models.load_model(model_dir + '/alexNet_best_weigths.hdf5')\n",
    "drop_layer = 0\n",
    "for i in range(len(model.layers)-1, 0, -1):\n",
    "    name = model.layers[i].name.split('_')[0]\n",
    "    if(name == 'flatten'):\n",
    "        drop_layer+= 1\n",
    "        break\n",
    "    drop_layer += 1\n",
    "new_model = tf.keras.Sequential()\n",
    "for layer in model.layers[:-drop_layer]:\n",
    "    new_model.add(layer)\n",
    "new_model.trainable = False\n",
    "new_model.build((227,227,3))\n",
    "new_model.summary()\n",
    "#train_feature = create_feature(train_df, new_model)\n",
    "# Grid Search\n",
    "# Use small dataset to make search much faster\n",
    "# acc_record = grid_search(train_df, train_feature)\n",
    "# print('Validation Record: ')\n",
    "# for key in acc_record.keys():\n",
    "#     print(key, ':', acc_record.get(key))\n",
    "\n",
    "# b = 1 will give prob est\n",
    "# highest params for food-101 dataset\n",
    "# (0, 2, 0.5)\n",
    "# train_model = svm_train(train_df['label'].tolist(), train_feature, '-s 0 -t 0 -g 2 -c 0.5 -b 1')\n",
    "# svm_save_model('all_food_classification.model', train_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n",
      "total image:  151\n",
      "Reshape: 151 6400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "data_dir_1 = \"/mnt/c/Users/nhmin/Downloads/food-102\"\n",
    "# load json file\n",
    "train_json_1 = load_json(data_dir_1 + '/meta/train.json')\n",
    "train_df_1 = create_data(train_json_1)\n",
    "print(len(train_df_1['filename']))\n",
    "train_feature_1 = create_feature(train_df_1, new_model)\n",
    "# import some data to play with\n",
    "# iris = datasets.load_iris()\n",
    "#X = iris.data[:, :2]  # we only take the first two features. We could\n",
    "                      # avoid this ugly slicing by using a two-dim dataset\n",
    "X = train_feature_1[:,:2]\n",
    "y = train_df_1['label'].tolist()\n",
    "\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 0.5  # SVM regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC(kernel='linear', C=C, gamma=2).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_svc = svm.SVC(kernel='rbf', gamma=2, C=C).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_svc = svm.SVC(kernel='poly', gamma=2, degree=3, C=C).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mesh to plot in\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['Linear kernel',\n",
    "          'Polynomial kernel',\n",
    "          'RBF kernel']\n",
    "\n",
    "\n",
    "for i, clf in enumerate((svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
