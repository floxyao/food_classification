{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 6: Summary\n",
    "\n",
    "This was my first ssuccessful iteration.  Using the model from the previous iteration, I trained for 30 epochs, with a batch size of 64, and using the Adam optimizer with a learn rate of 0.01.\n",
    "\n",
    "I was not able to get results from the test dataset, but the training ended with a validation accuracy of 0.68 and a validation loss of 0.72."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "source = './food-101/test/ramen'\n",
    "dest = './food-101/valid/ramen'\n",
    "\n",
    "files = os.listdir(source)\n",
    "\n",
    "i = 0\n",
    "for f in files:\n",
    "    if i < 175:\n",
    "        file= os.path.join(source, f)\n",
    "        shutil.move(file,dest)\n",
    "        print(\"adding img \", i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Loading Data\n",
    "\n",
    "For loading the data, I first used the train/test split that was provided with our dataset.  This gave us 750 images in train and 250 in test PER CLASS.  Then, I split the test set into test & validation.  I moved 175 images to the validation set, thus we are left with 75 images in test.\n",
    "\n",
    "For image preprocessing, I first resize each image to our IMG_SIZE constant, currently set to 224 pixels.  I then recolor the image to RGB.  I also assign the labels to the images and shuffle the dataset.\n",
    "\n",
    "\n",
    "After loading the data, the shape of X (features) will be (-1 {this means any number of features}, IMG_SIZE, IMG_SIZE, 3 {number of channels - RGB}), and the shape of y (labels) will be (-1, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import shutil\n",
    "import stat\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constants\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 5\n",
    "noodle_classes = ['pad_thai','pho','ramen','spaghetti_bolognese','spaghetti_carbonara']\n",
    "\n",
    "# these are the main variables we tested and documented\n",
    "EPOCHS = 30\n",
    "BATCH_SZ = 64\n",
    "LEARN_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to create the datasets (train, test, and valid)\n",
    "# preprocesses the images (resize/recolor), assigns labels, and shuffles the dataset\n",
    "root_dir = 'food-101/images/'\n",
    "\n",
    "def create_data(root, _name):\n",
    "    data_set = []\n",
    "\n",
    "    for noodle_class in noodle_classes:\n",
    "        path = os.path.join(root, noodle_class)\n",
    "        label = noodle_classes.index(noodle_class) # assign an integer label to the image based on our noodle_classes array\n",
    "        print(noodle_class,label)\n",
    "\n",
    "        for img in os.listdir(path): # iterates through each image in the noodle folder\n",
    "                if img.startswith('.'):\n",
    "                    continue\n",
    "                # each image is a 2D array of RGB value\n",
    "                try:\n",
    "                    img_arr = cv2.imread(os.path.join(path,img)) \n",
    "                    img_to_rgb = img_arr[:,:,::-1] # recolor\n",
    "                    new_img = cv2.resize(img_to_rgb,(IMG_SIZE,IMG_SIZE)) #resize\n",
    "                    data_set.append([new_img,label]) # store image and label together in dataset so we can shuffle without images getting mislabeled\n",
    "                except Exception as e: # catch bad images\n",
    "                    print(\"create exception: \",e)\n",
    "        \n",
    "    # randomize\n",
    "    random.shuffle(data_set)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for features, label in data_set:\n",
    "        X.append(features) # 2D array of RGB values representing features\n",
    "        y.append(label) # integer representing class/label\n",
    "\n",
    "    # reshape X and y\n",
    "    X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    print('X', X.shape)\n",
    "    print('y', y.shape)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (3750, 224, 224, 3)\n",
      "y (3750, 1)\n",
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (375, 224, 224, 3)\n",
      "y (375, 1)\n",
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (875, 224, 224, 3)\n",
      "y (875, 1)\n"
     ]
    }
   ],
   "source": [
    "# load data in train, test, and valid\n",
    "\n",
    "X_train, y_train = create_data('food-101/train', \"train\")\n",
    "X_test, y_test = create_data('food-101/test', \"test\")\n",
    "X_valid, y_valid = create_data('food-101/valid', \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization and Augmentation\n",
    "Here I am normalizing the data to scale our input training vectors. This will help improve accuracy and increase training speed.\n",
    "\n",
    "Currently, I am attempting to implement image augmentation to improve accuracy as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train before normal [[0]\n",
      " [3]\n",
      " [3]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [0]]\n",
      "y train after normal [[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n",
      "(3750, 5)\n"
     ]
    }
   ],
   "source": [
    "# normalize data\n",
    "# X_train = X_train/255.0\n",
    "# X_test = X_test/255.0\n",
    "\n",
    "#print('X train before normalize',X_train[1])\n",
    "print('y train before normal',y_train)\n",
    "# input('wait1')\n",
    "\n",
    "# L2-normalizes the given array, i.e., it makes the sum of squares of each element of the array to be equal to one\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "\n",
    "#print('X train after normal',X_train[1])\n",
    "print('y train after normal',y_train)\n",
    "\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "X_valid = tf.keras.utils.normalize(X_valid, axis=1)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, NUM_CLASSES)\n",
    "\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "print('y train after normal',y_train)\n",
    "\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# image augmentation - for better performance\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 90,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    brightness_range=[0.2,1.0],\n",
    "    zoom_range=[0.5,1.0]\n",
    "    #preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "datagener = datagen.flow(X_train, y_train, batch_size = BATCH_SZ, shuffle=True)\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# image augmentation - for better performance\n",
    "vdatagen = ImageDataGenerator(\n",
    "    rotation_range = 90,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    brightness_range=[0.2,1.0],\n",
    "    zoom_range=[0.5,1.0]\n",
    "    #preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "vdatagener = vdatagen.flow(X_valid, y_valid, batch_size = BATCH_SZ, shuffle=True)\n",
    "vdatagen.fit(X_valid)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Statistics train=%.3f (%.3f), test=%.3f (%.3f)' % (X_train.mean(), X_train.std(), X_test.mean(), X_test.std()))\n",
    "\n",
    "# generator that centers pixel values\n",
    "datagen = ImageDataGenerator(featurewise_center = True,\n",
    "                            featurewise_std_normalization = True)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "train_iterator = datagen.flow(X_train, y_train, batch_size=BATCH_SZ)\n",
    "test_iterator = datagen.flow(X_test, y_test, batch_size=BATCH_SZ)\n",
    "valid_iterator = datagen.flow(X_valid, y_valid, batch_size=BATCH_SZ)\n",
    "\n",
    "'''\n",
    "# show the effect on a single batch of samples\n",
    "iterator = datagen.flow(X_train,\n",
    "                        y_train,\n",
    "                        batch_size=64)\n",
    "\n",
    "# get a batch\n",
    "X_batch, y_batch = iterator.next()\n",
    "#pixel stats in the batch\n",
    "print(X_batch.shape, X_batch.mean(), X_batch.std())\n",
    "\n",
    "# show effect on entire dataset\n",
    "iterator = datagen.flow(X_train,\n",
    "                        y_train,\n",
    "                        batch_size = len(X_train),\n",
    "                        shuffle = False)\n",
    "\n",
    "# get a batch\n",
    "X_batch, y_batch = iterator.next()\n",
    "# pixel stats in the batch\n",
    "print(X_batch.shape, X_batch.mean(), X_batch.std())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 224, 224, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 112, 112, 128)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 56, 56, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 200704)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              205521920 \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 208,312,133\n",
      "Trainable params: 208,312,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, BatchNormalization, LeakyReLU\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# creating the actual model using the VGGNet16 architecture\n",
    "\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3) # 224x224x3 RGB image\n",
    "\n",
    "#Instantiate an empty model\n",
    "model = Sequential([\n",
    "Conv2D(64, (3, 3), input_shape=input_shape, kernel_initializer='glorot_normal', bias_initializer='zeros', padding='same'), # first two convolutional layers\n",
    "LeakyReLU(alpha=0.01),\n",
    "Conv2D(64, (3, 3), padding='same'),\n",
    "LeakyReLU(alpha=0.01),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(128, (3, 3), padding='same'), \n",
    "LeakyReLU(alpha=0.01),\n",
    "Conv2D(128, (3, 3), padding='same',), \n",
    "LeakyReLU(alpha=0.01),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(256, (3, 3), padding='same',),\n",
    "LeakyReLU(alpha=0.01),\n",
    "Conv2D(256, (3, 3), padding='same',),\n",
    "LeakyReLU(alpha=0.01),\n",
    "Conv2D(256, (3, 3), padding='same',),\n",
    "LeakyReLU(alpha=0.01),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)), \n",
    "Flatten(), \n",
    "Dense(1024), # fully connected layers\n",
    "LeakyReLU(alpha=0.01),\n",
    "Dense(1024),\n",
    "LeakyReLU(alpha=0.01),\n",
    "Dense(5, activation='softmax') # softmax output layer, 5 possible values/classes\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model, show_shapes=True, to_file='vgg_block.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 224, 224, 3)\n",
      "(3750, 5)\n",
      "Epoch 1/30\n",
      "58/58 [==============================] - 1178s 20s/step - loss: 20.3834 - accuracy: 0.2018 - val_loss: 1.5462 - val_accuracy: 0.2389\n",
      "Epoch 2/30\n",
      "58/58 [==============================] - 1139s 20s/step - loss: 1.5138 - accuracy: 0.3180 - val_loss: 1.4924 - val_accuracy: 0.4057\n",
      "Epoch 3/30\n",
      "58/58 [==============================] - 1157s 20s/step - loss: 1.4618 - accuracy: 0.3635 - val_loss: 1.4698 - val_accuracy: 0.3703\n",
      "Epoch 4/30\n",
      "58/58 [==============================] - 1154s 20s/step - loss: 1.4261 - accuracy: 0.3869 - val_loss: 1.2519 - val_accuracy: 0.4194\n",
      "Epoch 5/30\n",
      "58/58 [==============================] - 1158s 20s/step - loss: 1.3473 - accuracy: 0.4308 - val_loss: 1.4079 - val_accuracy: 0.4789\n",
      "Epoch 6/30\n",
      "58/58 [==============================] - 1151s 20s/step - loss: 1.3059 - accuracy: 0.4440 - val_loss: 1.1791 - val_accuracy: 0.4891\n",
      "Epoch 7/30\n",
      "58/58 [==============================] - 1163s 20s/step - loss: 1.2787 - accuracy: 0.4674 - val_loss: 1.2453 - val_accuracy: 0.4594\n",
      "Epoch 8/30\n",
      "58/58 [==============================] - 1157s 20s/step - loss: 1.2373 - accuracy: 0.4889 - val_loss: 1.1189 - val_accuracy: 0.5303\n",
      "Epoch 9/30\n",
      "58/58 [==============================] - 1158s 20s/step - loss: 1.2067 - accuracy: 0.5106 - val_loss: 1.2334 - val_accuracy: 0.5657\n",
      "Epoch 10/30\n",
      "58/58 [==============================] - 1157s 20s/step - loss: 1.2090 - accuracy: 0.5210 - val_loss: 1.3467 - val_accuracy: 0.5280\n",
      "Epoch 11/30\n",
      "58/58 [==============================] - 1149s 20s/step - loss: 1.1697 - accuracy: 0.5298 - val_loss: 1.3283 - val_accuracy: 0.5806\n",
      "Epoch 12/30\n",
      "58/58 [==============================] - 1147s 20s/step - loss: 1.1504 - accuracy: 0.5461 - val_loss: 1.0472 - val_accuracy: 0.5680\n",
      "Epoch 13/30\n",
      "58/58 [==============================] - 1157s 20s/step - loss: 1.1121 - accuracy: 0.5612 - val_loss: 0.9842 - val_accuracy: 0.5771\n",
      "Epoch 14/30\n",
      "58/58 [==============================] - 1141s 20s/step - loss: 1.2652 - accuracy: 0.4923 - val_loss: 1.1972 - val_accuracy: 0.4994\n",
      "Epoch 15/30\n",
      "58/58 [==============================] - 1144s 20s/step - loss: 1.1118 - accuracy: 0.5524 - val_loss: 1.2291 - val_accuracy: 0.5931\n",
      "Epoch 16/30\n",
      "58/58 [==============================] - 1147s 20s/step - loss: 1.0675 - accuracy: 0.5800 - val_loss: 0.9945 - val_accuracy: 0.6229\n",
      "Epoch 17/30\n",
      "58/58 [==============================] - 1147s 20s/step - loss: 1.0287 - accuracy: 0.5950 - val_loss: 1.0048 - val_accuracy: 0.5543\n",
      "Epoch 18/30\n",
      "58/58 [==============================] - 1142s 20s/step - loss: 1.0863 - accuracy: 0.5754 - val_loss: 1.0347 - val_accuracy: 0.6114\n",
      "Epoch 19/30\n",
      "58/58 [==============================] - 1153s 20s/step - loss: 1.0261 - accuracy: 0.6056 - val_loss: 1.0660 - val_accuracy: 0.5406\n",
      "Epoch 20/30\n",
      "58/58 [==============================] - 1152s 20s/step - loss: 1.0296 - accuracy: 0.5973 - val_loss: 0.6996 - val_accuracy: 0.6434\n",
      "Epoch 21/30\n",
      "58/58 [==============================] - 1148s 20s/step - loss: 0.9724 - accuracy: 0.6196 - val_loss: 1.0911 - val_accuracy: 0.6297\n",
      "Epoch 22/30\n",
      "58/58 [==============================] - 1155s 20s/step - loss: 0.9948 - accuracy: 0.5983 - val_loss: 0.8894 - val_accuracy: 0.6480\n",
      "Epoch 23/30\n",
      "58/58 [==============================] - 1140s 20s/step - loss: 0.9303 - accuracy: 0.6366 - val_loss: 0.9758 - val_accuracy: 0.6811\n",
      "Epoch 24/30\n",
      "58/58 [==============================] - 1154s 20s/step - loss: 0.9368 - accuracy: 0.6344 - val_loss: 0.7149 - val_accuracy: 0.7097\n",
      "Epoch 25/30\n",
      "58/58 [==============================] - 1146s 20s/step - loss: 0.9067 - accuracy: 0.6473 - val_loss: 1.0002 - val_accuracy: 0.6674\n",
      "Epoch 26/30\n",
      "58/58 [==============================] - 1149s 20s/step - loss: 0.9365 - accuracy: 0.6403 - val_loss: 0.7719 - val_accuracy: 0.6674\n",
      "Epoch 27/30\n",
      "58/58 [==============================] - 1139s 20s/step - loss: 0.8893 - accuracy: 0.6503 - val_loss: 0.8746 - val_accuracy: 0.6994\n",
      "Epoch 28/30\n",
      "58/58 [==============================] - 1141s 20s/step - loss: 0.8865 - accuracy: 0.6601 - val_loss: 0.8920 - val_accuracy: 0.6960\n",
      "Epoch 29/30\n",
      "58/58 [==============================] - 1148s 20s/step - loss: 0.8710 - accuracy: 0.6677 - val_loss: 0.8302 - val_accuracy: 0.6560\n",
      "Epoch 30/30\n",
      "58/58 [==============================] - 1146s 20s/step - loss: 0.8602 - accuracy: 0.6704 - val_loss: 0.7271 - val_accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# fitting model with image standardization using ImageDataGenerator\\nmodel.fit_generator(train_iterator,\\n                    steps_per_epoch=len(train_iterator),\\n                    epochs=EPOCHS)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compiling model and training\n",
    "\n",
    "opt = optimizers.SGD(lr=LEARN_RATE, momentum=.9) # momentum help accelerate gradient vectors in the right directions\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "'''\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='SGD', metrics=['accuracy'])\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "'''\n",
    "# fitting model without image augmentation\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=BATCH_SZ, \n",
    "          epochs=EPOCHS,     \n",
    "          validation_data=(X_valid, y_valid))\n",
    "'''\n",
    "\n",
    "# fitting model with image augmentation\n",
    "model.fit_generator(datagener,\n",
    "                    steps_per_epoch=X_train.shape[0] // BATCH_SZ,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=vdatagener,\n",
    "                    shuffle=True)\n",
    "\n",
    "'''\n",
    "# fitting model with image standardization using ImageDataGenerator\n",
    "model.fit_generator(train_iterator,\n",
    "                    steps_per_epoch=len(train_iterator),\n",
    "                    epochs=EPOCHS)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 30s 81ms/step\n",
      "Test loss: 2.1899173425038656\n",
      "Test accuracy: 0.20000000298023224\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on test data\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "# verbose indiciator to display training progress info\n",
    "# 1 (true) (default) | 0 (false)\n",
    "#scores = model.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image augmentation - for better performance\n",
    "tdatagen = ImageDataGenerator(\n",
    "    rotation_range = 90,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    brightness_range=[0.2,1.0],\n",
    "    zoom_range=[0.5,1.0]\n",
    "    #preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "tdatagener = vdatagen.flow(X_test, y_test, batch_size = BATCH_SZ)\n",
    "tdatagen.fit(X_test)\n",
    "\n",
    "scores = model.evaluate_generator(tdatagener, verbose=1)\n",
    "print('Test Loss: ', scores[0])\n",
    "print('Test Accuracy: ', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
