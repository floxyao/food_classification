{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 1: Summary\n",
    "\n",
    "This was my first successful run.  I used the VGGNet 16 architecture exactly with the SGD optimizer.  I used a learning rate of 0.0001, batch size 32, and epochs 30.\n",
    "\n",
    "Test results:\n",
    "\n",
    "Accuracy 0.4133\n",
    "Loss 1.6084"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport shutil\\nimport os\\n\\nsource = \\'./food-101/test/spaghetti_carbonara\\'\\ndest = \\'./food-101/valid/spaghetti_carbonara\\'\\n\\nfiles = os.listdir(source)\\n\\ni = 0\\nfor f in files:\\n    if i < 175:\\n        file= os.path.join(source, f)\\n        shutil.move(file,dest)\\n        print(\"adding img \", i)\\n    i+=1\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "source = './food-101/test/spaghetti_carbonara'\n",
    "dest = './food-101/valid/spaghetti_carbonara'\n",
    "\n",
    "files = os.listdir(source)\n",
    "\n",
    "i = 0\n",
    "for f in files:\n",
    "    if i < 175:\n",
    "        file= os.path.join(source, f)\n",
    "        shutil.move(file,dest)\n",
    "        print(\"adding img \", i)\n",
    "    i+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 1 RUN ME\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import shutil\n",
    "import stat\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 RUN ME\n",
    "# constants\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 5\n",
    "noodle_classes = ['pad_thai','pho','ramen','spaghetti_bolognese','spaghetti_carbonara']\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SZ = 32\n",
    "LEARN_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 RUN ME\n",
    "def prepare(filepath):\n",
    "    print('fp:',filepath)\n",
    "    try:\n",
    "        #img = os.listdir(filepath)\n",
    "        #print(img)\n",
    "        img_arr = cv2.imread(filepath) # each image is a 2-d array of RGB values\n",
    "        img_to_rgb = img_arr[:,:,::-1] # convert image to rgb\n",
    "        new_img = cv2.resize(img_to_rgb, (IMG_SIZE,IMG_SIZE)) # indexed to remove dtype=uint8\n",
    "        #plt.imshow(new_img)\n",
    "        #plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"create exception: \",e)\n",
    "    \n",
    "    new_img = np.array(new_img).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "    new_img = tf.cast(new_img,tf.float32)\n",
    "    #new_img = new_img.astype('float32')\n",
    "    #print(new_img.dtype)\n",
    "    #new_img = tf.convert_to_tensor(new_img,dtype=tf.float32)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 RUN ME\n",
    "root_dir = 'food-101/images/'\n",
    "\n",
    "# this function pre-processes every food image\n",
    "# and assigns label to the respective food class\n",
    "# shoves everything into an array called data_set\n",
    "# shuffles the array\n",
    "# then separates the data_set array into features array and label array\n",
    "def create_data(root, _name):\n",
    "    data_set = []\n",
    "\n",
    "    # pre-process: re-size and re-color\n",
    "    for noodle_class in noodle_classes:\n",
    "        path = os.path.join(root, noodle_class)\n",
    "        label = noodle_classes.index(noodle_class) # FIX LATER: use enumerate\n",
    "        print(noodle_class,label)\n",
    "        #input('wait1')\n",
    "\n",
    "        for img in os.listdir(path): # pad_thai/1432432.jpg, pad_thai/12314.jpg . . .\n",
    "                if img.startswith('.'): # .DS_Store\n",
    "                    continue\n",
    "                # each image is a 2D array of RGB value\n",
    "                try:\n",
    "                    img_arr = cv2.imread(os.path.join(path,img))\n",
    "                    img_to_rgb = img_arr[:,:,::-1] \n",
    "                    new_img = cv2.resize(img_to_rgb,(IMG_SIZE,IMG_SIZE)) \n",
    "                    data_set.append([new_img,label]) # shove (X,y) in here cause we wanna shuffle \n",
    "                except Exception as e:\n",
    "                    print(\"create exception: \",e)\n",
    "        \n",
    "    # randomize\n",
    "    random.shuffle(data_set)\n",
    "    random.shuffle(data_set)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # now separate X and y \n",
    "    # X (features) is a 2D array of RGB values\n",
    "    # y (labels) is a 1D array of integers\n",
    "    for features, label in data_set:\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "        #print('feature loop')\n",
    "        #print(features, label)\n",
    "        #input('wait')\n",
    "\n",
    "    # re-shape for training\n",
    "    X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3) # -1 = any number of features (catch-all, it'll recognize)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    print('X', X.shape)\n",
    "    print('y', y.shape)\n",
    "    \n",
    "    #save(X,y,_name)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (3750, 224, 224, 3)\n",
      "y (3750, 1)\n",
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (375, 224, 224, 3)\n",
      "y (375, 1)\n",
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (875, 224, 224, 3)\n",
      "y (875, 1)\n"
     ]
    }
   ],
   "source": [
    "# 5 RUN ME\n",
    "# load data\n",
    "\n",
    "X_train, y_train = create_data('food-101/train', \"train\")\n",
    "X_test, y_test = create_data('food-101/test', \"test\")\n",
    "X_valid, y_valid = create_data('food-101/valid', \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-527cb3c0b6b94e5f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-527cb3c0b6b94e5f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train before normal [[1]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [1]]\n",
      "y train after normal [[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 6 RUN ME\n",
    "# normalize (before feeding data into network)\n",
    "# X_train = X_train/255.0\n",
    "# X_test = X_test/255.0\n",
    "\n",
    "#print('X train before normalize',X_train[1])\n",
    "print('y train before normal',y_train)\n",
    "# input('wait1')\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "\n",
    "#print('X train after normal',X_train[1])\n",
    "print('y train after normal',y_train)\n",
    "# input('wait2')\n",
    "\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "#print('y test',y_test)\n",
    "\n",
    "X_valid = tf.keras.utils.normalize(X_valid, axis=1)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet CNN (Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 134,281,029\n",
      "Trainable params: 134,281,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(3750, 224, 224, 3)\n",
      "(3750, 5)\n",
      "Train on 3750 samples, validate on 875 samples\n",
      "Epoch 1/30\n",
      "3750/3750 [==============================] - 1063s 284ms/step - loss: 1.6094 - accuracy: 0.2053 - val_loss: 1.6094 - val_accuracy: 0.2914\n",
      "Epoch 2/30\n",
      "3750/3750 [==============================] - 1074s 286ms/step - loss: 1.6094 - accuracy: 0.2435 - val_loss: 1.6093 - val_accuracy: 0.2046\n",
      "Epoch 3/30\n",
      "3750/3750 [==============================] - 1073s 286ms/step - loss: 1.6093 - accuracy: 0.2264 - val_loss: 1.6093 - val_accuracy: 0.2937\n",
      "Epoch 4/30\n",
      "3750/3750 [==============================] - 1059s 282ms/step - loss: 1.6093 - accuracy: 0.2363 - val_loss: 1.6093 - val_accuracy: 0.4091\n",
      "Epoch 5/30\n",
      "3750/3750 [==============================] - 1077s 287ms/step - loss: 1.6093 - accuracy: 0.2352 - val_loss: 1.6092 - val_accuracy: 0.2537\n",
      "Epoch 6/30\n",
      "3750/3750 [==============================] - 1064s 284ms/step - loss: 1.6093 - accuracy: 0.2395 - val_loss: 1.6092 - val_accuracy: 0.2789\n",
      "Epoch 7/30\n",
      "3750/3750 [==============================] - 1080s 288ms/step - loss: 1.6093 - accuracy: 0.2536 - val_loss: 1.6092 - val_accuracy: 0.3829\n",
      "Epoch 8/30\n",
      "3750/3750 [==============================] - 1135s 303ms/step - loss: 1.6092 - accuracy: 0.3152 - val_loss: 1.6092 - val_accuracy: 0.2834\n",
      "Epoch 9/30\n",
      "3750/3750 [==============================] - 1134s 302ms/step - loss: 1.6092 - accuracy: 0.2731 - val_loss: 1.6091 - val_accuracy: 0.2686\n",
      "Epoch 10/30\n",
      "3750/3750 [==============================] - 1137s 303ms/step - loss: 1.6092 - accuracy: 0.3360 - val_loss: 1.6091 - val_accuracy: 0.4057\n",
      "Epoch 11/30\n",
      "3750/3750 [==============================] - 1132s 302ms/step - loss: 1.6092 - accuracy: 0.2875 - val_loss: 1.6091 - val_accuracy: 0.3577\n",
      "Epoch 12/30\n",
      "3750/3750 [==============================] - 1138s 304ms/step - loss: 1.6091 - accuracy: 0.2592 - val_loss: 1.6091 - val_accuracy: 0.3760\n",
      "Epoch 13/30\n",
      "3750/3750 [==============================] - 1139s 304ms/step - loss: 1.6091 - accuracy: 0.2853 - val_loss: 1.6090 - val_accuracy: 0.3817\n",
      "Epoch 14/30\n",
      "3750/3750 [==============================] - 1143s 305ms/step - loss: 1.6091 - accuracy: 0.2920 - val_loss: 1.6090 - val_accuracy: 0.4149\n",
      "Epoch 15/30\n",
      "3750/3750 [==============================] - 1113s 297ms/step - loss: 1.6091 - accuracy: 0.3013 - val_loss: 1.6090 - val_accuracy: 0.3269\n",
      "Epoch 16/30\n",
      "3750/3750 [==============================] - 1079s 288ms/step - loss: 1.6090 - accuracy: 0.3275 - val_loss: 1.6090 - val_accuracy: 0.3497\n",
      "Epoch 17/30\n",
      "3750/3750 [==============================] - 1075s 287ms/step - loss: 1.6090 - accuracy: 0.2987 - val_loss: 1.6089 - val_accuracy: 0.3817\n",
      "Epoch 18/30\n",
      "3750/3750 [==============================] - 1081s 288ms/step - loss: 1.6090 - accuracy: 0.3091 - val_loss: 1.6089 - val_accuracy: 0.3886\n",
      "Epoch 19/30\n",
      "3750/3750 [==============================] - 1073s 286ms/step - loss: 1.6089 - accuracy: 0.3227 - val_loss: 1.6089 - val_accuracy: 0.4114\n",
      "Epoch 20/30\n",
      "3750/3750 [==============================] - 1103s 294ms/step - loss: 1.6089 - accuracy: 0.3291 - val_loss: 1.6088 - val_accuracy: 0.3440\n",
      "Epoch 21/30\n",
      "3750/3750 [==============================] - 1131s 301ms/step - loss: 1.6089 - accuracy: 0.3229 - val_loss: 1.6088 - val_accuracy: 0.3657\n",
      "Epoch 22/30\n",
      "3750/3750 [==============================] - 1081s 288ms/step - loss: 1.6088 - accuracy: 0.3056 - val_loss: 1.6087 - val_accuracy: 0.3783\n",
      "Epoch 23/30\n",
      "3750/3750 [==============================] - 1072s 286ms/step - loss: 1.6088 - accuracy: 0.3565 - val_loss: 1.6087 - val_accuracy: 0.4080\n",
      "Epoch 24/30\n",
      "3750/3750 [==============================] - 1087s 290ms/step - loss: 1.6087 - accuracy: 0.3651 - val_loss: 1.6086 - val_accuracy: 0.3829\n",
      "Epoch 25/30\n",
      "3750/3750 [==============================] - 1077s 287ms/step - loss: 1.6087 - accuracy: 0.3123 - val_loss: 1.6086 - val_accuracy: 0.3829\n",
      "Epoch 26/30\n",
      "3750/3750 [==============================] - 1079s 288ms/step - loss: 1.6086 - accuracy: 0.3651 - val_loss: 1.6085 - val_accuracy: 0.4103\n",
      "Epoch 27/30\n",
      "3750/3750 [==============================] - 1076s 287ms/step - loss: 1.6086 - accuracy: 0.3584 - val_loss: 1.6085 - val_accuracy: 0.3589\n",
      "Epoch 28/30\n",
      "3750/3750 [==============================] - 1079s 288ms/step - loss: 1.6086 - accuracy: 0.3488 - val_loss: 1.6084 - val_accuracy: 0.3909\n",
      "Epoch 29/30\n",
      "3750/3750 [==============================] - 1075s 287ms/step - loss: 1.6085 - accuracy: 0.3360 - val_loss: 1.6084 - val_accuracy: 0.3886\n",
      "Epoch 30/30\n",
      "3750/3750 [==============================] - 1068s 285ms/step - loss: 1.6084 - accuracy: 0.3683 - val_loss: 1.6083 - val_accuracy: 0.3943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f765dbf3eb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 RUN ME - this is the one that takes hours\n",
    "## WORKING!!!!\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "#Instantiate an empty model\n",
    "model = Sequential([\n",
    "Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'),\n",
    "Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "Conv2D(128, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Flatten(),\n",
    "Dense(4096, activation='relu'),\n",
    "Dense(4096, activation='relu'),\n",
    "Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = optimizers.SGD(lr=LEARN_RATE, momentum=.9)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "'''\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='SGD', metrics=['accuracy'])\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=BATCH_SZ, \n",
    "          epochs=EPOCHS,     \n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 23s 61ms/step\n",
      "Test loss: 1.6084366137186685\n",
      "Test accuracy: 0.41333332657814026\n"
     ]
    }
   ],
   "source": [
    "# 8 RUN ME\n",
    "# evaluate model on test data\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "# verbose indiciator to display training progress info\n",
    "# 1 (true) (default) | 0 (false)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore below (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 112, 112, 64)\n",
      "(None, 56, 56, 128)\n",
      "(None, 28, 28, 256)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "=================================================================\n",
      "Total params: 2,325,568\n",
      "Trainable params: 2,325,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected max_pooling2d_8 to have 4 dimensions, but got array with shape (3750, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f91f63cf636d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# batch_size is how much data a time we wanna pass through a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# epochs = how many times go through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m           validation_split=(X_valid, y_valid))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected max_pooling2d_8 to have 4 dimensions, but got array with shape (3750, 5)"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOT WORKING\n",
    "# Example of creating a CNN model with many VGG blocks\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# function for creating a vgg block\n",
    "def vgg_block(layer_in, n_filters, n_conv):\n",
    "    # add convolutional layers\n",
    "    for _ in range(n_conv):\n",
    "        layer_in = Conv2D(n_filters, (3,3), padding='same', activation='relu')(layer_in)\n",
    "    # add max pooling layer\n",
    "    layer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n",
    "    print(layer_in.shape)\n",
    "    return layer_in\n",
    "\n",
    "# define model input\n",
    "visible = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "# add vgg module\n",
    "layer = vgg_block(visible, 64, 2)\n",
    "# add vgg module\n",
    "layer = vgg_block(layer, 128, 2)\n",
    "# add vgg module\n",
    "layer = vgg_block(layer, 256, 4)\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=visible, outputs=layer)\n",
    "# summarize model\n",
    "model.summary()\n",
    "# plot model architecture\n",
    "plot_model(model, show_shapes=True, to_file='multiple_vgg_blocks.png')\n",
    "\n",
    "opt = optimizers.SGD(lr=0.01, momentum=.9)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=32, # batch_size is how much data a time we wanna pass through a layer\n",
    "          epochs=20,      # epochs = how many times go through the network\n",
    "          validation_split=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image input shape:  (None, 128, 128, 3)\n",
      "layer 1, block1_conv1 shape:  (None, 128, 128, 64)\n",
      "last layer, block5_pool shape:  (None, 4, 4, 512)\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "X_train shape:  (3750, 128, 128, 3)\n",
      "y_train shape:  (3750, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (3750, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d7f411e579ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# batch_size is how much data a time we wanna pass through a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# epochs = how many times go through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m           validation_split=(X_valid, y_valid))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (3750, 5)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# NOT WORKING\n",
    "# from github\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "\n",
    "'''\n",
    "# function for creating a vgg block\n",
    "def vgg_block(layer_in, n_filters, n_conv):\n",
    "    # add convolutional layers\n",
    "    for _ in range(n_conv):\n",
    "        layer_in = Conv2D(n_filters, (3,3), padding='same', activation='relu')(layer_in)\n",
    "    # add max pooling layer\n",
    "    layer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n",
    "    print(layer_in.shape)\n",
    "    return layer_in\n",
    "    \n",
    "# specify model\n",
    "model = Sequential()\n",
    "\n",
    "# layer 1: input shape mandatory\n",
    "model.add(Conv2D(conv_filters_1, (3,3), strides=(1,1), padding='same',input_shape = X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "'''\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Block 1\n",
    "img_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "print(\"image input shape: \", img_input.shape)\n",
    "x = Conv2D(64, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block1_conv1')(img_input)\n",
    "print(\"layer 1, block1_conv1 shape: \", x.shape)\n",
    "x = Conv2D(64, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv2D(128, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block2_conv1')(x)\n",
    "x = Conv2D(128, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "# Block 3\n",
    "x = Conv2D(256, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv1')(x)\n",
    "x = Conv2D(256, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv2')(x)\n",
    "x = Conv2D(256, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv3')(x)\n",
    "x = Conv2D(256, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv4')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "# Block 4\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block4_conv1')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block4_conv2')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block4_conv3')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block4_conv4')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "# Block 5\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block5_conv1')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block5_conv2')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block5_conv3')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block5_conv4')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "print(\"last layer, block5_pool shape: \", x.shape)\n",
    "#x = keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "# Create model.\n",
    "model = Model(img_input, x, name='vgg19')\n",
    "\n",
    "#model = Model(inputs=img_input, outputs=x)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# plot model architecture\n",
    "plot_model(model, show_shapes=True, to_file='multiple_vgg_blocks.png')\n",
    "\n",
    "opt = optimizers.SGD(lr=0.01, momentum=.9)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=32, # batch_size is how much data a time we wanna pass through a layer\n",
    "          epochs=20,      # epochs = how many times go through the network\n",
    "          validation_split=(X_valid, y_valid))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate model on test data\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "# verbose indiciator to display training progress info\n",
    "# 1 (true) (default) | 0 (false)\n",
    "print(NAME)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify a random image\n",
    "#X = prepare('/Users/flo/Desktop/456/test/carb.jpg')\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/carb.jpg')]) \n",
    "# this prediction should give real values (percentages) for each class like 70% sure this class is this\n",
    "\n",
    "print(prediction.shape)\n",
    "prediction = model.predict_classes(X_test[:10])\n",
    "\n",
    "print('Predicted class: ')\n",
    "print(prediction)\n",
    "\n",
    "print('Real class: ')\n",
    "print(y_test[:10])\n",
    "\n",
    "model_accuracies = model.predict(X_test[:10])\n",
    "print('\\nAccuracy of each model')\n",
    "print(model_accuracies[:10])\n",
    "\n",
    "print('\\n(pretty)Accuracy of each model')\n",
    "print(np.round(model_accuracies[:10],3))\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/carb.jpg')])\n",
    "# print(\"carb:\", prediction)\n",
    "# print(noodle_classes[int(prediction[0][0])])\n",
    "\n",
    "\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/padthai.jpg')])\n",
    "# print(\"pdt:\", prediction)\n",
    "# print(noodle_classes[int(prediction[0][0])])\n",
    "\n",
    "\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/pho.jpg')])\n",
    "# print(\"ph:\", prediction)\n",
    "# print(noodle_classes[int(prediction[0][0])])\n",
    "\n",
    "\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/ramen.jpg')])\n",
    "# print(\"ram:\", prediction)\n",
    "# print(noodle_classes[int(prediction[0][0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#['spaghetti_bolognese', 'pad_thai', 'spaghetti_carbonara', 'ramen', 'pho']\n",
    "\n",
    "prediction = model.predict([prepare('/Users/flo/Desktop/456/test/carb.jpg')])\n",
    "#prediction2 = model.predict([prepare('/Users/flo/Desktop/sc.jpg')])\n",
    "\n",
    "print(prediction)\n",
    "#print(prediction2)\n",
    "#input('wait')\n",
    "#print(CATEGORIES[int(prediction[0][0])])\n",
    "#print(CATEGORIES[int(prediction2[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Right before the last layer. This is generally a bad place to apply dropout, \n",
    "#because the network has no ability to \"correct\" errors induced by dropout \n",
    "#before the classification happens. If I read correctly, you might have put \n",
    "#dropout right before the softmax in the iris MLP.\n",
    "\n",
    "#our data set provided us with the training data and test data [1:3] ratio"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
