{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iteration 2: Summary\n",
    "\n",
    "I kept the same learning rate as the last iteration, but changed the batch size to 64 and ran for 40 epochs.\n",
    "\n",
    "Test results:\n",
    "\n",
    "Accuracy 0.2987\n",
    "Loss 1.6089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport shutil\\nimport os\\n\\nsource = \\'./food-101/test/spaghetti_carbonara\\'\\ndest = \\'./food-101/valid/spaghetti_carbonara\\'\\n\\nfiles = os.listdir(source)\\n\\ni = 0\\nfor f in files:\\n    if i < 175:\\n        file= os.path.join(source, f)\\n        shutil.move(file,dest)\\n        print(\"adding img \", i)\\n    i+=1\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "source = './food-101/test/spaghetti_carbonara'\n",
    "dest = './food-101/valid/spaghetti_carbonara'\n",
    "\n",
    "files = os.listdir(source)\n",
    "\n",
    "i = 0\n",
    "for f in files:\n",
    "    if i < 175:\n",
    "        file= os.path.join(source, f)\n",
    "        shutil.move(file,dest)\n",
    "        print(\"adding img \", i)\n",
    "    i+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 1 RUN ME\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import shutil\n",
    "import stat\n",
    "\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 RUN ME\n",
    "# constants\n",
    "IMG_SIZE = 224\n",
    "NUM_CLASSES = 5\n",
    "noodle_classes = ['pad_thai','pho','ramen','spaghetti_bolognese','spaghetti_carbonara']\n",
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SZ = 64\n",
    "LEARN_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 RUN ME\n",
    "def prepare(filepath):\n",
    "    print('fp:',filepath)\n",
    "    try:\n",
    "        #img = os.listdir(filepath)\n",
    "        #print(img)\n",
    "        img_arr = cv2.imread(filepath) # each image is a 2-d array of RGB values\n",
    "        img_to_rgb = img_arr[:,:,::-1] # convert image to rgb\n",
    "        new_img = cv2.resize(img_to_rgb, (IMG_SIZE,IMG_SIZE)) # indexed to remove dtype=uint8\n",
    "        #plt.imshow(new_img)\n",
    "        #plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"create exception: \",e)\n",
    "    \n",
    "    new_img = np.array(new_img).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "    new_img = tf.cast(new_img,tf.float32)\n",
    "    #new_img = new_img.astype('float32')\n",
    "    #print(new_img.dtype)\n",
    "    #new_img = tf.convert_to_tensor(new_img,dtype=tf.float32)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 RUN ME\n",
    "root_dir = 'food-101/images/'\n",
    "\n",
    "# this function pre-processes every food image\n",
    "# and assigns label to the respective food class\n",
    "# shoves everything into an array called data_set\n",
    "# shuffles the array\n",
    "# then separates the data_set array into features array and label array\n",
    "def create_data(root, _name):\n",
    "    data_set = []\n",
    "\n",
    "    # pre-process: re-size and re-color\n",
    "    for noodle_class in noodle_classes:\n",
    "        path = os.path.join(root, noodle_class)\n",
    "        label = noodle_classes.index(noodle_class) # FIX LATER: use enumerate\n",
    "        print(noodle_class,label)\n",
    "        #input('wait1')\n",
    "\n",
    "        for img in os.listdir(path): # pad_thai/1432432.jpg, pad_thai/12314.jpg . . .\n",
    "                if img.startswith('.'): # .DS_Store\n",
    "                    continue\n",
    "                # each image is a 2D array of RGB value\n",
    "                try:\n",
    "                    img_arr = cv2.imread(os.path.join(path,img))\n",
    "                    img_to_rgb = img_arr[:,:,::-1] \n",
    "                    new_img = cv2.resize(img_to_rgb,(IMG_SIZE,IMG_SIZE)) \n",
    "                    data_set.append([new_img,label]) # shove (X,y) in here cause we wanna shuffle \n",
    "                except Exception as e:\n",
    "                    print(\"create exception: \",e)\n",
    "        \n",
    "    # randomize\n",
    "    random.shuffle(data_set)\n",
    "    random.shuffle(data_set)\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # now separate X and y \n",
    "    # X (features) is a 2D array of RGB values\n",
    "    # y (labels) is a 1D array of integers\n",
    "    for features, label in data_set:\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "        #print('feature loop')\n",
    "        #print(features, label)\n",
    "        #input('wait')\n",
    "\n",
    "    # re-shape for training\n",
    "    X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3) # -1 = any number of features (catch-all, it'll recognize)\n",
    "    y = np.array(y).reshape(-1, 1)\n",
    "    print('X', X.shape)\n",
    "    print('y', y.shape)\n",
    "    \n",
    "    #save(X,y,_name)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (3750, 224, 224, 3)\n",
      "y (3750, 1)\n",
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (375, 224, 224, 3)\n",
      "y (375, 1)\n",
      "pad_thai 0\n",
      "pho 1\n",
      "ramen 2\n",
      "spaghetti_bolognese 3\n",
      "spaghetti_carbonara 4\n",
      "X (875, 224, 224, 3)\n",
      "y (875, 1)\n"
     ]
    }
   ],
   "source": [
    "# 5 RUN ME\n",
    "# load data\n",
    "\n",
    "X_train, y_train = create_data('food-101/train', \"train\")\n",
    "X_test, y_test = create_data('food-101/test', \"test\")\n",
    "X_valid, y_valid = create_data('food-101/valid', \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-527cb3c0b6b94e5f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-527cb3c0b6b94e5f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train before normal [[4]\n",
      " [3]\n",
      " [0]\n",
      " ...\n",
      " [2]\n",
      " [0]\n",
      " [0]]\n",
      "y train after normal [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 6 RUN ME\n",
    "# normalize (before feeding data into network)\n",
    "# X_train = X_train/255.0\n",
    "# X_test = X_test/255.0\n",
    "\n",
    "#print('X train before normalize',X_train[1])\n",
    "print('y train before normal',y_train)\n",
    "# input('wait1')\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "\n",
    "#print('X train after normal',X_train[1])\n",
    "print('y train after normal',y_train)\n",
    "# input('wait2')\n",
    "\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "#print('y test',y_test)\n",
    "\n",
    "X_valid = tf.keras.utils.normalize(X_valid, axis=1)\n",
    "y_valid = tf.keras.utils.to_categorical(y_valid, NUM_CLASSES)b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet CNN (Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 20485     \n",
      "=================================================================\n",
      "Total params: 134,281,029\n",
      "Trainable params: 134,281,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "(3750, 224, 224, 3)\n",
      "(3750, 5)\n",
      "Train on 3750 samples, validate on 875 samples\n",
      "Epoch 1/40\n",
      "3750/3750 [==============================] - 930s 248ms/step - loss: 1.6094 - accuracy: 0.1939 - val_loss: 1.6094 - val_accuracy: 0.2480\n",
      "Epoch 2/40\n",
      "3750/3750 [==============================] - 925s 247ms/step - loss: 1.6094 - accuracy: 0.2456 - val_loss: 1.6094 - val_accuracy: 0.2891\n",
      "Epoch 3/40\n",
      "3750/3750 [==============================] - 917s 245ms/step - loss: 1.6094 - accuracy: 0.2723 - val_loss: 1.6094 - val_accuracy: 0.3566\n",
      "Epoch 4/40\n",
      "3750/3750 [==============================] - 920s 245ms/step - loss: 1.6094 - accuracy: 0.2189 - val_loss: 1.6093 - val_accuracy: 0.2377\n",
      "Epoch 5/40\n",
      "3750/3750 [==============================] - 919s 245ms/step - loss: 1.6093 - accuracy: 0.2952 - val_loss: 1.6093 - val_accuracy: 0.3120\n",
      "Epoch 6/40\n",
      "3750/3750 [==============================] - 918s 245ms/step - loss: 1.6093 - accuracy: 0.2907 - val_loss: 1.6093 - val_accuracy: 0.2789\n",
      "Epoch 7/40\n",
      "3750/3750 [==============================] - 921s 246ms/step - loss: 1.6093 - accuracy: 0.2675 - val_loss: 1.6093 - val_accuracy: 0.4251\n",
      "Epoch 8/40\n",
      "3750/3750 [==============================] - 921s 246ms/step - loss: 1.6093 - accuracy: 0.2397 - val_loss: 1.6093 - val_accuracy: 0.2366\n",
      "Epoch 9/40\n",
      "3750/3750 [==============================] - 928s 247ms/step - loss: 1.6093 - accuracy: 0.3595 - val_loss: 1.6093 - val_accuracy: 0.3543\n",
      "Epoch 10/40\n",
      "3750/3750 [==============================] - 923s 246ms/step - loss: 1.6093 - accuracy: 0.2480 - val_loss: 1.6093 - val_accuracy: 0.2274\n",
      "Epoch 11/40\n",
      "3750/3750 [==============================] - 922s 246ms/step - loss: 1.6093 - accuracy: 0.2869 - val_loss: 1.6093 - val_accuracy: 0.2457\n",
      "Epoch 12/40\n",
      "3750/3750 [==============================] - 925s 247ms/step - loss: 1.6093 - accuracy: 0.2715 - val_loss: 1.6092 - val_accuracy: 0.3851\n",
      "Epoch 13/40\n",
      "3750/3750 [==============================] - 925s 247ms/step - loss: 1.6093 - accuracy: 0.3496 - val_loss: 1.6092 - val_accuracy: 0.4171\n",
      "Epoch 14/40\n",
      "3750/3750 [==============================] - 918s 245ms/step - loss: 1.6093 - accuracy: 0.2603 - val_loss: 1.6092 - val_accuracy: 0.4080\n",
      "Epoch 15/40\n",
      "3750/3750 [==============================] - 919s 245ms/step - loss: 1.6092 - accuracy: 0.3416 - val_loss: 1.6092 - val_accuracy: 0.3166\n",
      "Epoch 16/40\n",
      "3750/3750 [==============================] - 918s 245ms/step - loss: 1.6092 - accuracy: 0.3173 - val_loss: 1.6092 - val_accuracy: 0.4480\n",
      "Epoch 17/40\n",
      "3750/3750 [==============================] - 922s 246ms/step - loss: 1.6092 - accuracy: 0.3237 - val_loss: 1.6092 - val_accuracy: 0.3646\n",
      "Epoch 18/40\n",
      "3750/3750 [==============================] - 915s 244ms/step - loss: 1.6092 - accuracy: 0.3213 - val_loss: 1.6092 - val_accuracy: 0.3863\n",
      "Epoch 19/40\n",
      "3750/3750 [==============================] - 916s 244ms/step - loss: 1.6092 - accuracy: 0.3147 - val_loss: 1.6092 - val_accuracy: 0.3634\n",
      "Epoch 20/40\n",
      "3750/3750 [==============================] - 922s 246ms/step - loss: 1.6092 - accuracy: 0.3267 - val_loss: 1.6091 - val_accuracy: 0.3989\n",
      "Epoch 21/40\n",
      "3750/3750 [==============================] - 921s 246ms/step - loss: 1.6092 - accuracy: 0.3048 - val_loss: 1.6091 - val_accuracy: 0.3383\n",
      "Epoch 22/40\n",
      "3750/3750 [==============================] - 923s 246ms/step - loss: 1.6091 - accuracy: 0.3131 - val_loss: 1.6091 - val_accuracy: 0.3223\n",
      "Epoch 23/40\n",
      "3750/3750 [==============================] - 918s 245ms/step - loss: 1.6091 - accuracy: 0.3368 - val_loss: 1.6091 - val_accuracy: 0.3029\n",
      "Epoch 24/40\n",
      "3750/3750 [==============================] - 919s 245ms/step - loss: 1.6091 - accuracy: 0.2877 - val_loss: 1.6091 - val_accuracy: 0.2663\n",
      "Epoch 25/40\n",
      "3750/3750 [==============================] - 915s 244ms/step - loss: 1.6091 - accuracy: 0.3200 - val_loss: 1.6091 - val_accuracy: 0.3634\n",
      "Epoch 26/40\n",
      "3750/3750 [==============================] - 922s 246ms/step - loss: 1.6091 - accuracy: 0.3403 - val_loss: 1.6091 - val_accuracy: 0.2697\n",
      "Epoch 27/40\n",
      "3750/3750 [==============================] - 918s 245ms/step - loss: 1.6091 - accuracy: 0.3451 - val_loss: 1.6090 - val_accuracy: 0.3383\n",
      "Epoch 28/40\n",
      "3750/3750 [==============================] - 920s 245ms/step - loss: 1.6091 - accuracy: 0.2987 - val_loss: 1.6090 - val_accuracy: 0.2960\n",
      "Epoch 29/40\n",
      "3750/3750 [==============================] - 919s 245ms/step - loss: 1.6091 - accuracy: 0.3371 - val_loss: 1.6090 - val_accuracy: 0.3943\n",
      "Epoch 30/40\n",
      "3750/3750 [==============================] - 917s 245ms/step - loss: 1.6090 - accuracy: 0.3837 - val_loss: 1.6090 - val_accuracy: 0.3840\n",
      "Epoch 31/40\n",
      "3750/3750 [==============================] - 921s 246ms/step - loss: 1.6090 - accuracy: 0.3635 - val_loss: 1.6090 - val_accuracy: 0.3520\n",
      "Epoch 32/40\n",
      "3750/3750 [==============================] - 917s 245ms/step - loss: 1.6090 - accuracy: 0.3040 - val_loss: 1.6090 - val_accuracy: 0.4011\n",
      "Epoch 33/40\n",
      "3750/3750 [==============================] - 922s 246ms/step - loss: 1.6090 - accuracy: 0.3093 - val_loss: 1.6089 - val_accuracy: 0.3600\n",
      "Epoch 34/40\n",
      "3750/3750 [==============================] - 921s 246ms/step - loss: 1.6090 - accuracy: 0.3411 - val_loss: 1.6089 - val_accuracy: 0.4194\n",
      "Epoch 35/40\n",
      "3750/3750 [==============================] - 920s 245ms/step - loss: 1.6089 - accuracy: 0.3595 - val_loss: 1.6089 - val_accuracy: 0.3909\n",
      "Epoch 36/40\n",
      "3750/3750 [==============================] - 923s 246ms/step - loss: 1.6089 - accuracy: 0.3187 - val_loss: 1.6089 - val_accuracy: 0.4057\n",
      "Epoch 37/40\n",
      "3750/3750 [==============================] - 920s 245ms/step - loss: 1.6089 - accuracy: 0.3243 - val_loss: 1.6089 - val_accuracy: 0.3531\n",
      "Epoch 38/40\n",
      "3750/3750 [==============================] - 914s 244ms/step - loss: 1.6089 - accuracy: 0.3445 - val_loss: 1.6088 - val_accuracy: 0.4011\n",
      "Epoch 39/40\n",
      "3750/3750 [==============================] - 925s 247ms/step - loss: 1.6089 - accuracy: 0.3355 - val_loss: 1.6088 - val_accuracy: 0.3977\n",
      "Epoch 40/40\n",
      "3750/3750 [==============================] - 921s 246ms/step - loss: 1.6089 - accuracy: 0.3661 - val_loss: 1.6088 - val_accuracy: 0.2983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fa51c8826d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 RUN ME - this is the one that takes hours\n",
    "## WORKING!!!!\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "#Instantiate an empty model\n",
    "model = Sequential([\n",
    "Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu'),\n",
    "Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "Conv2D(128, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(256, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "Conv2D(512, (3, 3), activation='relu', padding='same',),\n",
    "MaxPooling2D(pool_size=(2, 2), strides=(2, 2)),\n",
    "Flatten(),\n",
    "Dense(4096, activation='relu'),\n",
    "Dense(4096, activation='relu'),\n",
    "Dense(5, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "opt = optimizers.SGD(lr=LEARN_RATE, momentum=.9)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "'''\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='SGD', metrics=['accuracy'])\n",
    "'''\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=BATCH_SZ, \n",
    "          epochs=EPOCHS,     \n",
    "          validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 23s 61ms/step\n",
      "Test loss: 1.608901547431946\n",
      "Test accuracy: 0.29866665601730347\n"
     ]
    }
   ],
   "source": [
    "# 8 RUN ME\n",
    "# evaluate model on test data\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "# verbose indiciator to display training progress info\n",
    "# 1 (true) (default) | 0 (false)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore below (not working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 112, 112, 64)\n",
      "(None, 56, 56, 128)\n",
      "(None, 28, 28, 256)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "=================================================================\n",
      "Total params: 2,325,568\n",
      "Trainable params: 2,325,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected max_pooling2d_8 to have 4 dimensions, but got array with shape (3750, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f91f63cf636d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# batch_size is how much data a time we wanna pass through a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# epochs = how many times go through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m           validation_split=(X_valid, y_valid))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected max_pooling2d_8 to have 4 dimensions, but got array with shape (3750, 5)"
     ]
    }
   ],
   "source": [
    "\n",
    "# NOT WORKING\n",
    "# Example of creating a CNN model with many VGG blocks\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "\n",
    "# function for creating a vgg block\n",
    "def vgg_block(layer_in, n_filters, n_conv):\n",
    "    # add convolutional layers\n",
    "    for _ in range(n_conv):\n",
    "        layer_in = Conv2D(n_filters, (3,3), padding='same', activation='relu')(layer_in)\n",
    "    # add max pooling layer\n",
    "    layer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n",
    "    print(layer_in.shape)\n",
    "    return layer_in\n",
    "\n",
    "# define model input\n",
    "visible = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "# add vgg module\n",
    "layer = vgg_block(visible, 64, 2)\n",
    "# add vgg module\n",
    "layer = vgg_block(layer, 128, 2)\n",
    "# add vgg module\n",
    "layer = vgg_block(layer, 256, 4)\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=visible, outputs=layer)\n",
    "# summarize model\n",
    "model.summary()\n",
    "# plot model architecture\n",
    "plot_model(model, show_shapes=True, to_file='multiple_vgg_blocks.png')\n",
    "\n",
    "opt = optimizers.SGD(lr=0.01, momentum=.9)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=32, # batch_size is how much data a time we wanna pass through a layer\n",
    "          epochs=20,      # epochs = how many times go through the network\n",
    "          validation_split=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image input shape:  (None, 128, 128, 3)\n",
      "layer 1, block1_conv1 shape:  (None, 128, 128, 64)\n",
      "last layer, block5_pool shape:  (None, 4, 4, 512)\n",
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "X_train shape:  (3750, 128, 128, 3)\n",
      "y_train shape:  (3750, 5)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (3750, 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d7f411e579ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    139\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# batch_size is how much data a time we wanna pass through a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m      \u001b[0;31m# epochs = how many times go through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m           validation_split=(X_valid, y_valid))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected block5_pool to have 4 dimensions, but got array with shape (3750, 5)"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# NOT WORKING\n",
    "# from github\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.utils import plot_model\n",
    "\n",
    "'''\n",
    "# function for creating a vgg block\n",
    "def vgg_block(layer_in, n_filters, n_conv):\n",
    "    # add convolutional layers\n",
    "    for _ in range(n_conv):\n",
    "        layer_in = Conv2D(n_filters, (3,3), padding='same', activation='relu')(layer_in)\n",
    "    # add max pooling layer\n",
    "    layer_in = MaxPooling2D((2,2), strides=(2,2))(layer_in)\n",
    "    print(layer_in.shape)\n",
    "    return layer_in\n",
    "    \n",
    "# specify model\n",
    "model = Sequential()\n",
    "\n",
    "# layer 1: input shape mandatory\n",
    "model.add(Conv2D(conv_filters_1, (3,3), strides=(1,1), padding='same',input_shape = X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "'''\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Block 1\n",
    "img_input = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "print(\"image input shape: \", img_input.shape)\n",
    "x = Conv2D(64, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block1_conv1')(img_input)\n",
    "print(\"layer 1, block1_conv1 shape: \", x.shape)\n",
    "x = Conv2D(64, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv2D(128, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block2_conv1')(x)\n",
    "x = Conv2D(128, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "# Block 3\n",
    "x = Conv2D(256, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv1')(x)\n",
    "x = Conv2D(256, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv2')(x)\n",
    "x = Conv2D(256, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv3')(x)\n",
    "x = Conv2D(256, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block3_conv4')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "# Block 4\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block4_conv1')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block4_conv2')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block4_conv3')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block4_conv4')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "# Block 5\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block5_conv1')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block5_conv2')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block5_conv3')(x)\n",
    "x = Conv2D(512, (3, 3),\n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  name='block5_conv4')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "print(\"last layer, block5_pool shape: \", x.shape)\n",
    "#x = keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "# Create model.\n",
    "model = Model(img_input, x, name='vgg19')\n",
    "\n",
    "#model = Model(inputs=img_input, outputs=x)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# plot model architecture\n",
    "plot_model(model, show_shapes=True, to_file='multiple_vgg_blocks.png')\n",
    "\n",
    "opt = optimizers.SGD(lr=0.01, momentum=.9)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=32, # batch_size is how much data a time we wanna pass through a layer\n",
    "          epochs=20,      # epochs = how many times go through the network\n",
    "          validation_split=(X_valid, y_valid))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate model on test data\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "# verbose indiciator to display training progress info\n",
    "# 1 (true) (default) | 0 (false)\n",
    "print(NAME)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classify a random image\n",
    "#X = prepare('/Users/flo/Desktop/456/test/carb.jpg')\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/carb.jpg')]) \n",
    "# this prediction should give real values (percentages) for each class like 70% sure this class is this\n",
    "\n",
    "print(prediction.shape)\n",
    "prediction = model.predict_classes(X_test[:10])\n",
    "\n",
    "print('Predicted class: ')\n",
    "print(prediction)\n",
    "\n",
    "print('Real class: ')\n",
    "print(y_test[:10])\n",
    "\n",
    "model_accuracies = model.predict(X_test[:10])\n",
    "print('\\nAccuracy of each model')\n",
    "print(model_accuracies[:10])\n",
    "\n",
    "print('\\n(pretty)Accuracy of each model')\n",
    "print(np.round(model_accuracies[:10],3))\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/carb.jpg')])\n",
    "# print(\"carb:\", prediction)\n",
    "# print(noodle_classes[int(prediction[0][0])])\n",
    "\n",
    "\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/padthai.jpg')])\n",
    "# print(\"pdt:\", prediction)\n",
    "# print(noodle_classes[int(prediction[0][0])])\n",
    "\n",
    "\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/pho.jpg')])\n",
    "# print(\"ph:\", prediction)\n",
    "# print(noodle_classes[int(prediction[0][0])])\n",
    "\n",
    "\n",
    "# prediction = model.predict([prepare('/Users/flo/Desktop/456/test/ramen.jpg')])\n",
    "# print(\"ram:\", prediction)\n",
    "# print(noodle_classes[int(prediction[0][0])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#['spaghetti_bolognese', 'pad_thai', 'spaghetti_carbonara', 'ramen', 'pho']\n",
    "\n",
    "prediction = model.predict([prepare('/Users/flo/Desktop/456/test/carb.jpg')])\n",
    "#prediction2 = model.predict([prepare('/Users/flo/Desktop/sc.jpg')])\n",
    "\n",
    "print(prediction)\n",
    "#print(prediction2)\n",
    "#input('wait')\n",
    "#print(CATEGORIES[int(prediction[0][0])])\n",
    "#print(CATEGORIES[int(prediction2[0][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Right before the last layer. This is generally a bad place to apply dropout, \n",
    "#because the network has no ability to \"correct\" errors induced by dropout \n",
    "#before the classification happens. If I read correctly, you might have put \n",
    "#dropout right before the softmax in the iris MLP.\n",
    "\n",
    "#our data set provided us with the training data and test data [1:3] ratio"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
